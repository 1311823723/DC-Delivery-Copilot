import json
import os
import glob
import docx2txt  # <--- 1. å¼•å…¥è¿™ä¸ªæ–°åº“
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# é…ç½®è·¯å¾„
DOCS_DIR = "../documents"
OUTPUT_FILE = "../public/knowledge_index.json"


def main():
    print("ðŸš€ å¼€å§‹æž„å»ºçŸ¥è¯†åº“...")

    # 2. è¯»å–æ–‡æ¡£ (æ”¯æŒ txt, md, AND docx!)
    # æ‰«æç›®å½•ä¸‹æ‰€æœ‰çš„ txt, md å’Œ docx æ–‡ä»¶
    files = glob.glob(os.path.join(DOCS_DIR, "*.md")) + \
            glob.glob(os.path.join(DOCS_DIR, "*.txt")) + \
            glob.glob(os.path.join(DOCS_DIR, "*.docx"))  # <--- 2. å¢žåŠ  docx æ‰«æ

    if not files:
        print(f"âš ï¸  åœ¨ {DOCS_DIR} æ²¡æ‰¾åˆ°æ–‡æ¡£ï¼Œè¯·æ”¾å…¥ .docx, .md æˆ– .txt æ–‡ä»¶")
        return

    all_text = ""
    for f in files:
        try:
            print(f"ðŸ“„ æ­£åœ¨è¯»å–: {os.path.basename(f)}...")

            # --- æ ¸å¿ƒæ”¹åŠ¨å¼€å§‹ ---
            if f.endswith(".docx"):
                # å¦‚æžœæ˜¯ Word æ–‡æ¡£ï¼Œç”¨ docx2txt è¯»å–
                text = docx2txt.process(f)
                all_text += text + "\n"
            else:
                # å¦‚æžœæ˜¯æ™®é€šæ–‡æœ¬ï¼Œç”¨æ ‡å‡†æ–¹å¼è¯»å–
                with open(f, 'r', encoding='utf-8') as file:
                    all_text += file.read() + "\n"
            # --- æ ¸å¿ƒæ”¹åŠ¨ç»“æŸ ---

            print(f"âœ… å·²åŠ è½½: {os.path.basename(f)}")
        except Exception as e:
            print(f"âŒ è¯»å–å¤±è´¥ {f}: {e}")

    # 3. æ–‡æœ¬åˆ‡ç‰‡ (Chunking)
    print("âœ‚ï¸  æ­£åœ¨åˆ‡åˆ†æ–‡æœ¬...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ";"]
    )
    chunks = text_splitter.create_documents([all_text])
    print(f"wb å…±åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")

    # 4. å‘é‡åŒ– (Embedding)
    print("ðŸ§  æ­£åœ¨è¿›è¡Œå‘é‡åŒ–è®¡ç®— (Loading model...)...")
    # ä¾ç„¶ä½¿ç”¨è¿™ä¸ªå…è´¹å¥½ç”¨çš„æœ¬åœ°æ¨¡åž‹
    embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    knowledge_base = []

    for i, chunk in enumerate(chunks):
        # è¿™é‡Œçš„ embed_query å¯èƒ½ä¼šèŠ±ä¸€ç‚¹æ—¶é—´ï¼Œå–å†³äºŽæ–‡æ¡£é•¿åº¦
        vector = embeddings_model.embed_query(chunk.page_content)
        knowledge_base.append({
            "id": i,
            "content": chunk.page_content,
            "vector": vector,
            "source": "å¼€å‘æ‰‹å†Œåº“"
        })

    # 5. ä¿å­˜
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(knowledge_base, f, ensure_ascii=False)

    print(f"ðŸŽ‰ æˆåŠŸï¼çŸ¥è¯†åº“å·²ç”Ÿæˆè‡³: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()