import json
import os
import glob
import fitz  # PyMuPDF
import docx2txt
from rapidocr_onnxruntime import RapidOCR
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
# === é…ç½®åŒºåŸŸ ===
# æ–‡æ¡£å­˜æ”¾ç›®å½•
DOCS_DIR = "../documents"
# è¾“å‡ºçš„å‘é‡åº“æ–‡ä»¶ (ç»™å‰ç«¯ç”¨çš„ä¼ªæ•°æ®åº“)
OUTPUT_FILE = "../public/knowledge_index.json"

# åˆå§‹åŒ– OCR å¼•æ“
# é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œç¨å¾®ç­‰ä¸€ä¸‹
ocr_engine = RapidOCR()


def extract_pdf_content(pdf_path):
    """
    æ·±åº¦è§£æ PDFï¼š
    1. æå–åŸç”Ÿæ–‡æœ¬
    2. æå–å›¾ç‰‡å¹¶è¿›è¡Œ OCR è¯†åˆ« (ä¸“æ²»æ¶æ„å›¾å’ŒæŠ¥é”™æˆªå›¾)
    """
    doc = fitz.open(pdf_path)
    full_text = ""
    print(f"    ...æ­£åœ¨æ·±åº¦è§£æ PDF (å«OCR): {os.path.basename(pdf_path)}")

    for i, page in enumerate(doc):
        # 1. æå–é¡µé¢åŸç”Ÿæ–‡æœ¬
        text = page.get_text()
        full_text += text + "\n"

        # 2. æå–é¡µé¢å†…çš„å›¾ç‰‡å¹¶è¿›è¡Œ OCR
        image_list = page.get_images(full=True)
        if image_list:
            # print(f"      - ç¬¬ {i+1} é¡µå‘ç° {len(image_list)} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨è¯†åˆ«...")
            for img_index, img in enumerate(image_list):
                try:
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]

                    # è°ƒç”¨ RapidOCR è¯†åˆ«
                    ocr_result, _ = ocr_engine(image_bytes)

                    if ocr_result:
                        # ocr_result æ˜¯åˆ—è¡¨ï¼Œæ‹¼æ¥æ‰€æœ‰è¯†åˆ«å‡ºçš„æ–‡å­—
                        img_text = " ".join([res[1] for res in ocr_result])
                        if img_text.strip():
                            # ç»™å›¾ç‰‡å†…å®¹åŠ ä¸ªç‰¹æ®Šçš„æ ‡è®°ï¼Œæ–¹ä¾¿è°ƒè¯•å’Œæ£€ç´¢
                            full_text += f"\n>>> [ç¬¬{i + 1}é¡µÂ·æ¶æ„å›¾/æˆªå›¾è¯†åˆ«]: {img_text}\n"
                except Exception as e:
                    pass  # å›¾ç‰‡è¯†åˆ«å¤±è´¥å°±ä¸ç®¡äº†ï¼Œç»§ç»­

    return full_text


def main():
    print("ğŸš€ å¼€å§‹æ„å»ºå¤šæ¨¡æ€å‘é‡çŸ¥è¯†åº“...")

    # 1. æ‰«ææ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹
    files = glob.glob(os.path.join(DOCS_DIR, "*.md")) + \
            glob.glob(os.path.join(DOCS_DIR, "*.txt")) + \
            glob.glob(os.path.join(DOCS_DIR, "*.docx")) + \
            glob.glob(os.path.join(DOCS_DIR, "*.pdf"))

    if not files:
        print(f"âš ï¸  åœ¨ {DOCS_DIR} æ²¡æ‰¾åˆ°æ–‡æ¡£ï¼Œè¯·æ”¾å…¥ .pdf, .docx, .md æˆ– .txt æ–‡ä»¶")
        return

    all_text = ""

    # 2. é€ä¸ªè¯»å–æ–‡ä»¶å†…å®¹
    for f in files:
        try:
            print(f"ğŸ“„ æ­£åœ¨è¯»å–: {os.path.basename(f)}...")

            if f.endswith(".docx"):
                text = docx2txt.process(f)
                all_text += text + "\n"

            elif f.endswith(".pdf"):
                # ä½¿ç”¨ä¸Šé¢çš„ OCR å¢å¼ºå‡½æ•°
                text = extract_pdf_content(f)
                all_text += text + "\n"

            else:
                # æ™®é€šæ–‡æœ¬
                with open(f, 'r', encoding='utf-8') as file:
                    all_text += file.read() + "\n"

            print(f"âœ… å·²åŠ è½½: {os.path.basename(f)}")
        except Exception as e:
            print(f"âŒ è¯»å–å¤±è´¥ {f}: {e}")

    # 3. æ–‡æœ¬åˆ‡ç‰‡ (Chunking)
    print("âœ‚ï¸  æ­£åœ¨åˆ‡åˆ†æ–‡æœ¬...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,  # æ¯ä¸ªç‰‡æ®µçš„å¤§å°
        chunk_overlap=100,  # é‡å éƒ¨åˆ†ï¼Œé˜²æ­¢åˆ‡æ–­ä¸Šä¸‹æ–‡
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ">>>"]  # æŠŠæˆ‘ä»¬åˆšæ‰åŠ çš„å›¾ç‰‡æ ‡è®°ä¹Ÿä½œä¸ºåˆ†éš”ç¬¦
    )
    chunks = text_splitter.create_documents([all_text])
    print(f"ğŸ“Š å…±åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")

    # 4. å‘é‡åŒ– (Embedding)
    print("ğŸ§  æ­£åœ¨è®¡ç®—å‘é‡ (åŠ è½½æ¨¡å‹å¯èƒ½éœ€è¦å‡ åç§’)...")
    # ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼Œä¸éœ€è¦ GPU ä¹Ÿèƒ½è·‘
    embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    knowledge_base = []

    for i, chunk in enumerate(chunks):
        # è®¡ç®—å‘é‡
        vector = embeddings_model.embed_query(chunk.page_content)
        knowledge_base.append({
            "id": i,
            "content": chunk.page_content,
            "vector": vector,
            "source": "Core_Knowledge_Base"
        })

    # 5. ä¿å­˜ä¸º JSON (å……å½“å‘é‡æ•°æ®åº“)
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(knowledge_base, f, ensure_ascii=False)

    print(f"ğŸ‰ æˆåŠŸï¼çŸ¥è¯†åº“å·²ç”Ÿæˆè‡³: {OUTPUT_FILE}")
    print("ğŸ‘‰ ç°åœ¨ä½ å¯ä»¥å»è¿è¡Œå‰ç«¯ä»£ç äº†ï¼Œå®ƒä¼šè‡ªåŠ¨è¯»å–è¿™ä¸ªæ–‡ä»¶ï¼")


if __name__ == "__main__":
    main()
